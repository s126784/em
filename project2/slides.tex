\documentclass{beamer}
\usetheme{Madrid}
\usecolortheme{whale}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,fit,calc}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{amsmath}
\usepackage{graphicx}

% Add footer template with logo on the left and slide numbers on the right
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.1\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \includegraphics[height=2ex]{ua_logo.png}
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.8\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.1\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
    \usebeamerfont{date in head/foot}\insertframenumber{}/\inserttotalframenumber\hspace*{2ex}
  \end{beamercolorbox}}%
  \vskip0pt%
}

\title{CDPCA with Missing Data: Evaluating Imputation Methods in Financial Markets}
\author[123155/123177/126784]{Syed Muhammad Zeeshan Bukhari ( 123155 )\\
Silvia Mastracci ( 123177 )\\
Oleksandr Solovei ( 126784 )
}
\date{November 19, 2024}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}{Introduction}
    \begin{block}{Problem Statement}
        \begin{itemize}
            \item High dimensionality of financial market data
            \item Complex patterns into different market sectors
            \item Missing data in financial time series adds another layer of complexity
            \item Challenge in identifying distinct market patterns
        \end{itemize}
    \end{block}

    \begin{block}{Objectives}
        \begin{itemize}
            \item Apply CDPCA to financial market data
            \item Introduce a new element by integrating missing data handling into the CDPCA framework.
            \item Address the challenge of missing data using different techniques
            \item Identify and interpret patterns across different market sectors
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Motivation}
    \begin{block}{Why CDPCA for Financial Markets?}
        \begin{itemize}
            \item PCA: Components can be hard to interpret due to mixed loadings
            \item K-means alone: Misses underlying market structure
            \item Need: Clear sector-based patterns for investment decisions
        \end{itemize}
    \end{block}

    \begin{block}{Key Advantages}
        \begin{itemize}
            \item Disjoint components: Each financial variable belongs to exactly one component
            \item Simultaneous clustering: Groups similar market sectors together
            \item Enhanced interpretability: Clear variable-based patterns within sectors
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{CDPCA Model}
    \begin{block}{Mathematical Framework}
        \begin{equation*}
            \mathbf{X} = \mathbf{U}\hat{\mathbf{Y}}\mathbf{A}' + \mathbf{E}
        \end{equation*}
        where:
        \begin{itemize}
        \item $\mathbf{X}$: Matrix of variables $(I \times J)$
        \item $\mathbf{U}$: Sector cluster membership $(I \times P)$
        \item $\hat{\mathbf{Y}}$: Cluster centroids $(P \times Q)$
        \item $\mathbf{A}$: Component loading matrix transposed $(Q \times J)$
        \item $\mathbf{E}$: Error matrix $(I \times J)$
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{CDPCA Algorithm}
    \begin{block}{Alternating Least Squares (ALS) Steps}
    \begin{enumerate}
        \item Update sector clusters ($\mathbf{U}$)
        \item Calculate cluster centroids ($\hat{\mathbf{Y}}$)
        \item Update component loadings ($\mathbf{A}$)
        \item Repeat until convergence
    \end{enumerate}
    \end{block}

    \begin{block}{Optimization Problem}
        Maximize between-cluster variance:
        \begin{equation*}
            \max_{\mathbf{U},\hat{\mathbf{Y}},\mathbf{A}} \|\mathbf{U}\hat{\mathbf{Y}}\mathbf{A}'\|^2
        \end{equation*}
    \end{block}
\end{frame}


\begin{frame}{CDPCA Visualization: Process Flow}
    \centering
    \includegraphics[width=0.9\textwidth,height=0.7\textheight,keepaspectratio]{cdpca-chart.png}
\end{frame}


\begin{frame}
    \frametitle{Introducing Missing Data Challenge}

    \begin{itemize}
        \item \textbf{Challenge: Missing Data in Time Series}
        \begin{itemize}
            \item In financial and stock market data, missing values are common due to various reasons, such as:
            \begin{itemize}
                \item Stock market holidays
                \item Errors in data collection or reporting
                \item Partial data availability across different time series
            \end{itemize}
            \item These missing values can significantly impact the results of principal component analysis (PCA), and even more so for CDPCA.
        \end{itemize}

    \end{itemize}
\end{frame}

\begin{frame}{Data Overview}

    \begin{block}{Data Source and Structure}
        \begin{itemize}
            \item Source: Yahoo Finance (quantmod R)
            \item Period: 2022-01-01 to 2023-12-31 (252 trading days/year)
            \item Daily market data for 9 stocks
        \end{itemize}
    \end{block}

    \begin{block}{Variables and Properties}
        \begin{description}\setlength{\itemsep}{1pt}
            \item[Price] Open, High, Low, Close, Adjusted Close (adjusted for corporate actions)
            \item[Volume] Daily trading volume (number of shares traded)
        \end{description}
        \vspace{4mm}
        \begin{itemize}
            \item Data standardized, missing values removed
            \item Dimensions: Rows (trading days), Columns (6 variables)
        \end{itemize}
    \end{block}


\end{frame}


\begin{frame}{Data Structure \& Coverage}
    \begin{columns}
        % Left Column
        \begin{column}{0.48\textwidth}
            \begin{block}{Dataset Overview}
                \begin{itemize}
                    \item \textbf{Observations}: 4,509 total
                    \item \textbf{Period}: 501 trading days
                    \item \textbf{Variables}: 6 per stock
                    \item \textbf{Missing Values}: None after cleaning
                \end{itemize}
            \end{block}

            \begin{block}{Variable Statistics}
                \begin{itemize}\setlength{\itemsep}{0pt}
                    \item All variables standardized
                    \item \textbf{Volume Range}: -0.91 to 6.79
                    \item \textbf{Price Range}: -1.33 to 2.57
                    \item High correlations within price variables
                \end{itemize}
            \end{block}
        \end{column}

        % Right Column
        \begin{column}{0.48\textwidth}
            \begin{block}{Market Sectors}
                \textbf{Technology} (NASDAQ)
                \begin{itemize}\setlength{\itemsep}{0pt}
                    \item AAPL: Apple Inc
                    \item MSFT: Microsoft Corp
                    \item GOOGL: Alphabet Inc
                \end{itemize}
                \textbf{Finance} (NYSE)
                \begin{itemize}\setlength{\itemsep}{0pt}
                    \item JPM: JPMorgan Chase
                    \item V: Visa Inc
                    \item MA: Mastercard Inc
                \end{itemize}
                \textbf{Consumer} (NYSE)
                \begin{itemize}\setlength{\itemsep}{0pt}
                    \item KO: Coca-Cola Co
                    \item PG: Procter \& Gamble
                    \item WMT: Walmart Inc
                \end{itemize}
            \end{block}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Variable Structure}
   \begin{tikzpicture}
       \begin{axis}[
           width=\textwidth,
           height=0.7\textheight,
           xlabel=Variables,
           ylabel=Loading Values,
           title=CDPCA Variable Loadings,
           symbolic x coords={Open, High, Low, Close, Volume, Adjusted},
           xtick=data,
           xticklabel style={rotate=45, anchor=east},
           legend pos=north west,
           ybar,
           bar width=7pt,
           ymin=0,
           ymax=1
       ]

       \addplot[fill=blue!40] coordinates {
           (Open,0.4469)
           (High,0.4470)
           (Low,0.4467)
           (Close,0.4469)
           (Volume,0.0000)
           (Adjusted,0.4486)
       };

       \addplot[fill=red!40] coordinates {
           (Open,0.0000)
           (High,0.0000)
           (Low,0.0000)
           (Close,0.0000)
           (Volume,1.0000)
           (Adjusted,0.0000)
       };

       \legend{Component 1, Component 2}
       \end{axis}
   \end{tikzpicture}
\end{frame}


\begin{frame}
    \frametitle{Handling Missing Data in CDPCA}


    \begin{itemize}
        \item Imputation: Replacing missing values with statistical estimates (mean, median, or regression-based methods).
        \item Model-based Methods: Using algorithms that can handle missing data directly (e.g., Expectation-Maximization, Bayesian approaches).
        \item Data Filtering: Removing rows or columns with excessive missing values (but potentially losing information).
    \end{itemize}


\end{frame}


\begin{frame}{Mean Imputation}
   \begin{block}{Original Data}
       Stock prices for 5 days:
       \[
       X = [100, 102, \text{NA}, 103, 101]
       \]
   \end{block}

   \begin{block}{Step-by-step Imputation}
       \begin{enumerate}
           \item Sum non-missing values:\\
           $100 + 102 + 103 + 101 = 406$
           \item Calculate mean using formula $\frac{\sum_{i=1}^{n} x_i}{n}$:\\
           $\bar{x} = \frac{406}{4} = 101.5$
           \item Replace NA:\\
           $X_{\text{imputed}} = [100, 102, \mathbf{101.5}, 103, 101]$
       \end{enumerate}
   \end{block}
\end{frame}

\begin{frame}{Median Imputation}
   \begin{block}{Original Data}
       Stock prices for 5 days:
       \[
       X = [100, 102, \text{NA}, 103, 101]
       \]
   \end{block}

   \begin{block}{Step-by-step Imputation}
       \begin{enumerate}
           \item Arrange non-missing values in ascending order:\\
           $[100, 101, 102, 103]$
           \item Identify the median:\\
           For $n=4$, the median is the average of the middle two values:\\
           $\text{Median} = \frac{101 + 102}{2} = 101.5$
           \item Replace NA:\\
           $X_{\text{imputed}} = [100, 102, \mathbf{101.5}, 103, 101]$
       \end{enumerate}
   \end{block}
\end{frame}

\begin{frame}{K-Nearest Neighbors (KNN) Imputation}
   \begin{block}{Original Data}
       Stock prices for 5 days:
       \[
       X = [100, 102, \text{NA}, 103, 101]
       \]
   \end{block}

   \begin{block}{Step-by-step Imputation}
       \begin{enumerate}
           \item Choose $k=2$ nearest neighbors based on the most similar observations in the dataset:
           \begin{itemize}
               \item Consider values close in time or other similar variables.
               \item Here, neighbors are $102$ and $103$ (values adjacent to the missing value in this example).
           \end{itemize}
           \item Calculate the mean of $k$ nearest neighbors:\\
           $\hat{x} = \frac{102 + 103}{2} = 102.5$
           \item Replace NA:\\
           $X_{\text{imputed}} = [100, 102, \mathbf{102.5}, 103, 101]$
       \end{enumerate}
   \end{block}

   % \begin{block}{Key Notes}
   %     \begin{itemize}
   %         \item KNN considers similarity between rows (e.g., based on distances in a multidimensional feature space).
   %         \item Effective for capturing local patterns but computationally intensive for large datasets.
   %     \end{itemize}
   % \end{block}
\end{frame}

\begin{frame}{Expectation-Maximization (EM) Imputation}
   \begin{block}{Original Data}
       Stock prices for 5 days:
       \[
       X = [100, 102, \text{NA}, 103, 101]
       \]
   \end{block}

   \begin{block}{Step-by-step Imputation}
       \begin{enumerate}
           \item Assign an initial guess for $\text{NA}$ (e.g., the mean of observed values):
           $X_{\text{init}} = [100, 102, 101.5, 103, 101]$.

           \item Expectation: Use the observed data and current estimates to compute statistical parameters (mean $\mu$, variance $\sigma^2$).
           Example: $\mu = 101.9$, $\sigma^2 = 1.56$.

           \item Maximization: Update the missing value based on these parameters. Replace $\text{NA}$ with the expected value conditioned on $\mu$ and $\sigma^2$:\\
           $X_{\text{updated}} = [100, 102, \mathbf{101.9}, 103, 101]$.

           \item Repeat Expectation and Maximization steps until convergence.
       \end{enumerate}
   \end{block}

   % \begin{block}{Key Notes}
   %     \begin{itemize}
   %         \item Iterative approach ensures convergence to a statistically consistent estimate.
   %         \item Assumes data follows an underlying distribution (e.g., Gaussian).
   %         \item Suitable for complex datasets but computationally intensive.
   %     \end{itemize}
   % \end{block}
\end{frame}

\begin{frame}{Data Filtering}
   \begin{block}{Original Data}
       Stock prices for 6 days:
       \[
       X = [100, 102, \text{NA}, 103, \text{NA}, 101]
       \]
   \end{block}

   \begin{block}{Approach: Filter Out Missing Data}
       \begin{enumerate}
           \item Identify Missing Values:
           Locate the positions of missing data:
           $X_{\text{NA}} = [\text{Index 3}, \text{Index 5}]$.

           \item Remove Rows with Missing Values:
           Exclude any rows (or days) with $\text{NA}$ values:
           $X_{\text{filtered}} = [100, 102, 103, 101]$.

           \item Resulting Dataset:
           Filtered data only contains complete records:
           \[
           X_{\text{filtered}} = [100, 102, 103, 101]
           \]
       \end{enumerate}
   \end{block}

   % \begin{block}{Key Notes}
   %     \begin{itemize}
   %         \item Pros: Simple to implement; ensures clean data.
   %         \item Cons: May lead to loss of valuable information, especially if many entries are missing.
   %         \item When to Use: Best for datasets with sparse missing values.
   %     \end{itemize}
   % \end{block}
\end{frame}


\begin{frame}{Method Comparison}
    \begin{block}{Key Findings}
        \begin{itemize}
            \item Data Filtering: Highest cluster deviance but information loss
            \item EM: Best balance between deviance and error
            \item Mean/Median: Similar performance, simple implementation
            \item KNN: Good for preserving local structure
        \end{itemize}
    \end{block}

    \begin{block}{Recommendations}
        \begin{itemize}
            \item Use EM for comprehensive analysis
            \item Consider KNN for pattern preservation
            \item Data Filtering when $quality > quantity$
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Experimental Setup}
    \begin{block}{Dataset Configuration}
        \begin{itemize}
            \item Stocks: 9 (3 sectors × 3 stocks)
            \item Variables per stock: 6
            \item Time period: 2022-2023
            \item Missing rate: 5\%
        \end{itemize}
    \end{block}

    \begin{block}{CDPCA Parameters}
        \begin{itemize}
            \item Number of clusters (P): 3
            \item Number of components (Q): 2
            \item Tolerance: $10^{-5}$
            \item Maximum iterations: 100
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Implementation Details}
    \begin{block}{R Implementation}
        \begin{itemize}
            \item Libraries: quantmod, Amelia, VIM
            \item Data Structure: 501 trading days × 6 variables
            \item Missing Data: 5\% randomly introduced
        \end{itemize}
    \end{block}

    \begin{block}{Key Functions}
        \begin{itemize}
            \item \texttt{CDpca()}: Core CDPCA algorithm
            \item \texttt{get\_stock\_data()}: Data acquisition
            \item \texttt{amelia()}: EM imputation
            \item \texttt{kNN()}: KNN imputation
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Implementation Example}
    \begin{block}{R Code for Missing Data Handling}
        \begin{verbatim}
# Create missing data pattern
missing_count <- floor(n * p * 0.05)
missing_indices <- sample(1:(n*p), missing_count)
X_missing[missing_indices] <- NA

# Apply EM imputation
X_em <- amelia(X_missing, m=1)$imputations[[1]]

# Run CDPCA
cdpc_results <- CDpca(X_em, P=3, Q=2,
                      tol=1e-5, maxit=100)
        \end{verbatim}
    \end{block}
\end{frame}


\begin{frame}{Imputation Results}
   \begin{tikzpicture}
       \begin{axis}[
           width=\textwidth,
           height=0.7\textheight,
           xlabel=Method,
           title=CDPCA Metrics by Imputation Method,
           symbolic x coords={Original,Mean,Median,KNN,EM,Filtered},
           xtick=data,
           xticklabel style={rotate=45, anchor=east},
           ylabel=Cluster Deviance (\%),
           legend pos=north east,
           ybar=0pt,
           bar width=7pt,
           ymin=70,
           ymax=75,
           axis y line*=left,
       ]

       \addplot[fill=blue!40] coordinates {
           (Original,73.95)
           (Mean,72.21)
           (Median,72.56)
           (KNN,74.09)
           (EM,73.48)
           (Filtered,73.99)
       };

       \end{axis}

       \begin{axis}[
           width=\textwidth,
           height=0.7\textheight,
           symbolic x coords={Original,Mean,Median,KNN,EM,Filtered},
           ylabel=Error Norm,
           ymin=0.015,
           ymax=0.03,
           axis y line*=right,
           axis x line=none,
       ]

       \addplot[red,only marks,mark=*,mark size=3pt] coordinates {
           (Original,0.01862)
           (Mean,0.02086)
           (Median,0.02084)
           (KNN,0.01858)
           (EM,0.01879)
           (Filtered,0.02558)
       };

       \legend{Error Norm}

       \end{axis}
   \end{tikzpicture}
\end{frame}

\begin{frame}{Cluster Quality Analysis: Silhouette Scores}
    \begin{tikzpicture}
        \begin{axis}[
            width=\textwidth,
            height=0.6\textheight,
            xlabel=Method,
            ylabel=Silhouette Score,
            symbolic x coords={Original,Mean,Median,KNN,EM,Filtered},
            xtick=data,
            xticklabel style={rotate=45, anchor=east},
            ybar,
            bar width=15pt,
            ymin=0.3,
            ymax=0.6,
            nodes near coords,
            nodes near coords style={font=\small},
            every node near coord/.append style={rotate=0, anchor=west}
        ]

        \addplot[fill=blue!40] coordinates {
            (Original,0.526)
            (Mean,0.464)
            (Median,0.476)
            (KNN,0.527)
            (EM,0.433)
            (Filtered,0.527)
        };

        \end{axis}
    \end{tikzpicture}

    \begin{itemize}
        \item KNN and Filtered methods maintain original cluster quality (0.527)
        \item All methods show moderate cluster structure ($> 0.4$)
        \item EM shows lowest cluster separation but still acceptable
    \end{itemize}
\end{frame}

\begin{frame}{Detailed Method Comparison}
    \begin{table}
        \centering
        \begin{tabular}{lcccccc}
            \hline
            \textbf{Method} & \textbf{Deviance} & \textbf{Error} & \textbf{Silhouette} & \textbf{Time} & \textbf{Memory} \\
            \hline
            Original & 73.95\% & 0.01862 & 0.526 & - & - \\
            Mean & 72.21\% & 0.02086 & 0.464 & Fast & Low \\
            Median & 72.56\% & 0.02084 & 0.476 & Fast & Low \\
            KNN & 74.09\% & 0.01858 & 0.527 & Medium & Medium \\
            EM & 73.48\% & 0.01879 & 0.433 & Slow & High \\
            Filtered & 73.99\% & 0.02558 & 0.527 & Fast & Low \\
            \hline
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}{Summary: Missing Data in CDPCA}
    \begin{block}{Key Findings}
        \begin{itemize}
        \item EM Imputation: Best balance between deviance and error
        \item KNN: Maintained original cluster quality (0.527 silhouette score)
        \item Data Filtering: High cluster deviance but information loss
        \item Mean/Median: Simple but less effective for complex patterns
        \end{itemize}
    \end{block}
    \begin{block}{Recommendations}
        \begin{itemize}
        \item Use EM imputation for comprehensive market analysis
        \item Consider KNN when pattern preservation is critical
        \item Avoid simple mean/median imputation for complex financial data
        \item Use filtering only when data quality is priority over quantity
        \end{itemize}
    \end{block}
\end{frame}

\end{document}
